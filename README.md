# ğŸš€ Ctrl_Alt_db â€” Airflow MariaDB Connector

**Team Name:** Ctrl_Alt_db  
**Project Title:** Airflow MariaDB Connector  
**Theme:** Integration  

---

## ğŸ§© Problem Statement

Apache Airflow currently lacks **native integration with MariaDB**, forcing developers to rely on the MySQL connector.  
However, this connector is **incompatible with key MariaDB-specific features** such as:

- âš¡ **ColumnStore**
- ğŸ“¥ **cpimport**
- ğŸ§  **Native JSON functions**

This limitation results in **reduced functionality** and **performance bottlenecks** in ETL workflows.  
Data pipelines built on Airflow cannot fully leverage **MariaDBâ€™s high-performance architecture**.

> ğŸ” **Benchmark Insight:**  
> The MariaDB Python connector outperforms the MySQL connector by up to **3Ã—** in operations like:
> - `executemany`
> - `SELECT`
> - `JSON_INSERT`

The absence of a native Airflowâ€“MariaDB connector thus limits Airflowâ€™s ability to orchestrate **modern, high-performance, and scalable MariaDB data workflows**.

---

## ğŸ’¡ Solution Overview

The **Airflow MariaDB Connector** introduces **seamless, native integration** between **Apache Airflow** and **MariaDB (including ColumnStore)**.

### âœ… Key Features

- ğŸ§© **Native Airflow connection type** for direct MariaDB integration (no MySQL fallback)
- ğŸš€ **High-speed data ingestion** using `cpimport`, optimized for bulk ETL operations
- ğŸ”„ **ETL workflows**: download â†’ transform â†’ load between **MariaDB** and **S3**
- ğŸ“Š **Columnar architecture support** for faster analytical queries
- âš™ï¸ **3Ã— performance improvement** over MySQL connector for critical database operations

---

## ğŸ§  Concept

### ğŸ¯ Goal
Build a **seamless ETL integration** between **Apache Airflow** and **MariaDB ColumnStore**.

### ğŸ’­ Idea
Automate **OpenFlights** data ingestion using:
- **Airflow DAGs** for orchestration
- **Secure SSH transfers**
- **cpimport** for high-performance bulk loading into ColumnStore

---

## ğŸ—ï¸ Principles & Design

| Principle   | Description |
|--------------|-------------|
| ğŸ” **Automation** | Entire data pipeline runs automatically via Airflow scheduling |
| ğŸ” **Security** | Uses SSH-based file transfer â€” no direct DB exposure |
| âš–ï¸ **Scalability** | ColumnStore ensures distributed & parallel data loading |
| ğŸ§© **Modularity** | Each dataset (airports, airlines, routes, etc.) is processed independently |
| ğŸ”„ **Reusability** | DAG supports adding new datasets via simple JSON config updates |

---

## âš™ï¸ Installation & Setup Instructions

Follow these steps to set up and run the **Airflow MariaDB Connector project** locally.

---

### ğŸ§± Step 1: Add Custom Provider to Airflow

Since Airflow doesnâ€™t natively support MariaDB, we created a **custom provider**.

This provider:
- Adds **MariaDB connection type**
- Provides **S3 hooks** and **cpimport operators**
- Enables **direct integration** with MariaDB from Airflow DAGs

```bash
# Clone or copy your provider into the airflow directory
COPY ./airflow-mariadb-provider /opt/airflow/.local/src/airflow-mariadb-provider

# Install the provider
pip install -e /opt/airflow/.local/src/airflow-mariadb-provider
```

### ğŸ¬ Step 2: Install MariaDB with ColumnStore Engine

We use MariaDB ColumnStore inside Docker for high-performance analytical queries.

```bash
docker run -d -p 3307:3306 -p 2222:22 --shm-size=512m -e PM1=mcs1 --hostname=mcs1 mariadb/columnstore
docker exec -it mcs1 provision mcs1
```

ğŸ§  Why ColumnStore?
It enables parallelized columnar data storage â€” perfect for analytical workloads.

### âš“ Step 3: Connect Airflow to MariaDB via Docker Network

```bash
docker network connect airflow_net mcs1
docker-compose down -v
docker-compose up -d
```

Your docker-compose.yml connects both containers (Airflow + MariaDB) via the same network for smooth communication.

### ğŸ³ Step 4: Dockerfile for Airflow with MariaDB Connector

```bash
# Use the official Airflow image
FROM apache/airflow:2.9.0

USER root

# Install dependencies for MariaDB
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc \
        libmariadb-dev \
        mariadb-client && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

USER airflow
ENV PATH="/home/airflow/.local/bin:${PATH}"

# Install Python MariaDB driver
RUN pip install --no-cache-dir mariadb

# Copy and install custom provider
COPY --chown=airflow:airflow ./airflow-mariadb-provider /opt/airflow/.local/src/airflow-mariadb-provider
RUN pip install --no-cache-dir -e /opt/airflow/.local/src/airflow-mariadb-provider
```

### ğŸ” Step 5: Enable SSH Connection in MariaDB Container

SSH is used for secure file transfers (e.g., CSV â†’ cpimport).

```bash
docker exec -it mcs1 bash
ssh-keygen -A
/usr/sbin/sshd -D &
exit
```

Then restart the Airflow webserver:

```bash
docker restart airflow-docker-airflow-webserver-1
```