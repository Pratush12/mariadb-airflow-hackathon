# ğŸš€ Ctrl_Alt_db â€” Airflow MariaDB Connector

**Team Name:** Ctrl_Alt_db  
**Project Title:** Airflow MariaDB Connector  
**Theme:** Integration  

---

## ğŸ§© Problem Statement

Apache Airflow currently lacks **native integration with MariaDB**, forcing developers to rely on the MySQL connector.  
However, this connector is **incompatible with key MariaDB-specific features** such as:

- âš¡ **ColumnStore**
- ğŸ“¥ **cpimport**
- ğŸ§  **Native JSON functions**

This limitation results in **reduced functionality** and **performance bottlenecks** in ETL workflows.  
Data pipelines built on Airflow cannot fully leverage **MariaDBâ€™s high-performance architecture**.

> ğŸ” **Benchmark Insight:**  
> The MariaDB Python connector outperforms the MySQL connector by up to **3Ã—** in operations like:
> - `executemany`
> - `SELECT`
> - `JSON_INSERT`

The absence of a native Airflowâ€“MariaDB connector thus limits Airflowâ€™s ability to orchestrate **modern, high-performance, and scalable MariaDB data workflows**.

---

## ğŸ’¡ Solution Overview

The **Airflow MariaDB Connector** introduces **seamless, native integration** between **Apache Airflow** and **MariaDB (including ColumnStore)**.

### âœ… Key Features

- ğŸ§© **Native Airflow connection type** for direct MariaDB integration (no MySQL fallback)
- ğŸš€ **High-speed data ingestion** using `cpimport`, optimized for bulk ETL operations
- ğŸ”„ **ETL workflows**: download â†’ transform â†’ load between **MariaDB** and **S3**
- ğŸ“Š **Columnar architecture support** for faster analytical queries
- âš™ï¸ **3Ã— performance improvement** over MySQL connector for critical database operations

---
## âš¡ Performance Comparison: MariaDB vs MySQL

We ran a quick benchmark using `mariadb_and_sql.py` to compare the **execution speed of common operations**.  
The results clearly demonstrate the **performance advantage of MariaDBâ€™s Python connector over MySQL**, especially for bulk inserts, SELECT queries, and JSON operations.

![MariaDB vs MySQL Performance Comparison](images/comparison_between_mariadb_mysql.png)

Note: 
- Above analysis was done on a local machine with 1Million records.
- Code ref: https://github.com/Pratush12/mariadb-airflow-hackathon/blob/main/mysql_vs_mariadb.py

---
## ğŸ§  Concept

### ğŸ¯ Goal
Build a **seamless ETL integration** between **Apache Airflow** and **MariaDB ColumnStore**.

### ğŸ’­ Idea
Automate **OpenFlights** data ingestion using:
- **Airflow DAGs** for orchestration
- **Secure SSH transfers**
- **cpimport** for high-performance bulk loading into ColumnStore

---

## ğŸ—ï¸ Principles & Design

| Principle   | Description |
|--------------|-------------|
| ğŸ” **Automation** | Entire data pipeline runs automatically via Airflow scheduling |
| ğŸ” **Security** | Uses SSH-based file transfer â€” no direct DB exposure |
| âš–ï¸ **Scalability** | ColumnStore ensures distributed & parallel data loading |
| ğŸ§© **Modularity** | Each dataset (airports, airlines, routes, etc.) is processed independently |
| ğŸ”„ **Reusability** | DAG supports adding new datasets via simple JSON config updates |

---

## âš™ï¸ Installation & Setup Instructions

Choose the installation method that best fits your needs:

---

### ğŸš€ Option 1: Quick Install (Recommended for Production)

Install the MariaDB provider directly from GitHub:

```bash
# Install the latest version
pip install -U git+https://github.com/Pratush12/mariadb-airflow-hackathon.git@main#subdirectory=airflow-mariadb-provider
```

**Requirements:**
- Python 3.8+
- Apache Airflow 2.5.0+
- MariaDB server (for database operations)
- SSH access (for cpimport operations)

---

### ğŸ”§ Option 2: Clone and Install (Development)

For development or customization:

```bash
# Clone the repository
git clone https://github.com/Pratush12/mariadb-airflow-hackathon.git
cd mariadb-airflow-hackathon/airflow-mariadb-provider

# Install in development mode
pip install -e ".[all]"

# Or install with specific features
pip install -e ".[amazon]"  # S3 support
pip install -e ".[ssh]"     # SSH support
```

---

### ğŸ³ Option 3: Docker Setup (Full Development Environment)

For a complete development environment with MariaDB ColumnStore:

#### ğŸ§± Step 1: Add Custom Provider to Airflow

Since Airflow doesn't natively support MariaDB, we created a **custom provider**.

This provider:
- Adds **MariaDB connection type**
- Provides **S3 hooks** and **cpimport operators**
- Enables **direct integration** with MariaDB from Airflow DAGs

```bash
# Clone or copy your provider into the airflow directory
COPY airflow-mariadb-provider /opt/airflow/airflow-mariadb-provider

# Install the provider
RUN pip install --no-deps /opt/airflow/airflow-mariadb-provider
```
---

## ğŸƒâ€â™‚ï¸ Quick Start

After installation, create a MariaDB connection in Airflow:

1. **Go to Admin â†’ Connections** in Airflow UI (`localhost:8080`)
2. **Add new connection:**
   - **Connection ID**: `maria_db_default`
   - **Connection Type**: `MariaDB`
   - **Host**: Your MariaDB server
   - **Port**: 3306
   - **Login**: Your username
   - **Password**: Your password
   - **Schema**: Your database name

3. **Use the operators in your DAGs:**
```python
from airflow.providers.mariadb.operators.mariadb import MariaDBOperator

# Basic SQL execution
sql_task = MariaDBOperator(
    task_id="execute_sql",
    mariadb_conn_id="maria_db_default",
    sql="SELECT * FROM my_table"
)
```

---

### ğŸ¬ Step 2: Install MariaDB with ColumnStore Engine

We use MariaDB ColumnStore inside Docker for high-performance analytical queries.

```bash
docker run -d -p 3307:3306 -p 2222:22 --shm-size=512m -e PM1=mcs1 --hostname=mcs1 mariadb/columnstore
docker exec -it mcs1 provision mcs1
```

ğŸ§  Why ColumnStore?
It enables parallelized columnar data storage â€” perfect for analytical workloads.

### âš“ Step 3: Connect Airflow to MariaDB via Docker Network

```bash
docker network connect airflow_net mcs1
docker-compose down -v
docker-compose up -d
```

Your docker-compose.yml connects both containers (Airflow + MariaDB) via the same network for smooth communication.

### ğŸ³ Step 4: Dockerfile for Airflow with MariaDB Connector

```bash
# Use the official Airflow image
FROM apache/airflow:2.9.0

# Switch to root user to install system dependencies
USER root

# Install system dependencies for MariaDB
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        gcc \
        libmariadb-dev \
        mariadb-client && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

USER airflow

# Set the PATH for the airflow user
ENV PATH="/home/airflow/.local/bin:${PATH}"

# Install Python dependencies (MariaDB driver)
RUN pip install --no-cache-dir mariadb

# Install your custom provider
# Make sure you install it without upgrading core Airflow packages
COPY airflow-mariadb-provider /opt/airflow/airflow-mariadb-provider
RUN pip install --no-deps /opt/airflow/airflow-mariadb-provider
```

### ğŸ” Step 5: Enable SSH Connection in MariaDB Container

SSH is used for secure file transfers (e.g., CSV â†’ cpimport).

```bash
docker exec -it mcs1 bash
ssh-keygen -A
/usr/sbin/sshd -D &
exit
```

Then restart the Airflow webserver:

```bash
docker restart airflow-docker-airflow-webserver-1
```

## ğŸ”— Airflow Connections

Once Airflow is running, configure these connections in the Airflow UI (`localhost:8080 â†’ Admin â†’ Connections`):

| Connection ID              | Type    | Description                                                                   |
|----------------------------| ------- |-------------------------------------------------------------------------------|
| `maria_db_default`         | MariaDB | Host: `hostname`, Port: `3306`, User: `user`, Password: `password`            |
| `mariadb_ssh_connection`   | SSH     | Host: `hostname`, Port: `22`, Username: `user`, Password: `password` |
| `aws_default` *(optional)* | S3      | For S3 data transfer workflows                                                |

---

## ğŸ“‚ Project Structure

Below is the overall structure of the project:

```
mariadb-airflow-hackathon/
â”‚
â”œâ”€ readme.MD # Project documentation
â”œâ”€ docker-compose.yml # Docker Compose setup for Airflow + MariaDB
â”œâ”€ Dockerfile # Custom Airflow image with MariaDB connector
â”œâ”€ requirements.txt # Python dependencies
â”œâ”€ mysql_vs_mariadb.py # Performance benchmarking script
â”œâ”€ images/ # Screenshots, performance charts
â”‚ â””â”€ comparison_between_mariadb_mysql.png
â”œâ”€ dags/ # Airflow DAGs
â”‚ â”œâ”€ config/
â”‚ â”‚ â””â”€ datasets.json # Dataset configuration
â”‚ â”œâ”€ openflights_dag.py # Main OpenFlights data ingestion DAG
â”‚ â”œâ”€ mariadb_s3_operators_dag.py # S3 integration DAG
â””â”€ airflow-mariadb-provider/ # Apache Airflow MariaDB Provider
   â”œâ”€ pyproject.toml # Modern Python packaging configuration
   â”œâ”€ README.rst # Provider documentation
   â”œâ”€ docs/ # Sphinx documentation
   â”‚ â”œâ”€ index.rst
   â”‚ â”œâ”€ changelog.rst
   â”‚ â”œâ”€ commits.rst
   â”‚ â”œâ”€ operators.rst
   â”‚ â”œâ”€ security.rst
   â”‚ â”œâ”€ connections/
   â”‚ â”‚ â””â”€ mariadb.rst
   â”‚ â”œâ”€ example_dags/
   â”‚ â”‚ â”œâ”€ example_mariadb_basic.py
   â”‚ â”‚ â”œâ”€ example_mariadb_cpimport.py
   â”‚ â”‚ â””â”€ example_mariadb_s3.py
   â”‚ â””â”€ installing-providers-from-sources.rst
   â”‚
   â”œâ”€ src/airflow/providers/mariadb/ # Main provider code
   â”‚ â”œâ”€ __init__.py # Provider metadata
   â”‚ â”œâ”€ get_provider_info.py # Provider information
   â”‚ â”œâ”€ hooks/
   â”‚ â”‚ â”œâ”€ __init__.py
   â”‚ â”‚ â””â”€ mariadb.py # MariaDB hook implementation
   â”‚ â”œâ”€ operators/
   â”‚ â”‚ â”œâ”€ __init__.py
   â”‚ â”‚ â”œâ”€ mariadb.py # Basic MariaDB operator
   â”‚ â”‚ â”œâ”€ cpimport.py # cpimport operator
   â”‚ â”‚ â””â”€ s3.py # S3 integration operators
   â”‚ â”œâ”€ sensors/
   â”‚ â”‚ â””â”€ __init__.py
   â”‚ â””â”€ transfers/
   â”‚     â””â”€ __init__.py
   â”‚
   â””â”€ tests/ # Comprehensive test suite
      â”œâ”€ conftest.py # Test configuration
      â”œâ”€ unit/mariadb/ # Unit tests
      â”‚ â”œâ”€ hooks/
      â”‚ â”‚ â””â”€ test_mariadb.py
      â”‚ â””â”€ __init__.py
      â””â”€ system/mariadb/ # System tests
         â”œâ”€ example_mariadb.py
         â””â”€ __init__.py
```

---
## ğŸ§ª Running the DAG

Start Airflow UI â†’ [http://localhost:8080](http://localhost:8080)

Trigger the DAG: **OpenFlights Data Ingestion**

Watch:

- ğŸ—‚ï¸ SSH file upload logs  
- âš™ï¸ cpimport execution  
- ğŸ“Š Data validation queries inside MariaDB  

---

## ğŸ§­ Outcome & Learnings

Through this project, we successfully:

- âœ… Built the first Airflowâ€“MariaDB native connector  
- âœ… Integrated MariaDB ColumnStore for parallel ETL  
- âœ… Learned Airflow provider development and Docker networking  
- âœ… Explored secure SSH integration for data transfer  
- âœ… Benchmarked MariaDB vs MySQL connector performance  

> ğŸ’¬ â€œThis hackathon gave us deep insights into how Airflow orchestrates ETL pipelines and how MariaDBâ€™s performance capabilities can be unlocked with the right integration.â€

---

## ğŸ Conclusion

The **Airflow MariaDB Connector** bridges a major integration gap in modern data engineering.  

It enables:

- âš¡ Direct and optimized Airflowâ€“MariaDB communication  
- ğŸš€ High-speed ETL via cpimport  
- ğŸ“ˆ Scalable analytics with ColumnStore  

With this, **Ctrl_Alt_db** has taken the first step toward empowering the Airflow community with a truly MariaDB-native data orchestration solution.
